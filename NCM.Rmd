---
title: 'Non-compulsory maize trial '
author: '[Michael Barber](mailto:mike.barber@oneacrefund.org)'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    code_folding: hide
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 6
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '6'
  css: styles.css
subtitle: Summary analysis of non-compulsory maize RCT, Kenya LR2017
---

```{r setup, include=FALSE}
#### set up
## clear environment and console
rm(list = ls())
cat("\014")




##getAnywhere(print.summary.lm)

## set up some global options
# define all knitr tables to be html format
options(knitr.table.format = 'html')

# change code chunk default to not show warnings or messages
knitr::opts_chunk$set(warning = FALSE, message = FALSE)

## load libraries
# dplyr and tibble are for working with tables
# reshape is for easy table transformation
# knitr is to make pretty tables at the end
# ggplot2 is for making graphs
# readxl is for reading in Excel files
library(plyr, quietly = T)  #data manipulation
library(dplyr, quietly = T, warn.conflicts = F, verbose = F)
library(reshape2, quietly = T)
library(sp, quietly = T)
library(MASS, quietly = T)
library(knitr, quietly = T)
library(ggplot2, quietly = T)
library(stargazer)
library(cowplot)
#library(boot)
library(tibble, quietly = T)
library(readxl, quietly = T)
library(FSA, quietly=T)
library(sfsmisc)
library(pwr)
library(robustbase)
library(stats)

library(lattice, quietly = T)
library(broom, quietly = T) #clean regression outputes
library(car)

library(forcats, quietly = T) #data manipulation
library(plotly, quietly = T) #interactive plots
library(readr, quietly = T) #faster FI



#### define helpful functions
# define function that turns decimal percentages into pretty formats
format_pct <- function(num) {round(num*100, digits = 2)}

#calc SEM of list
sems <- function(x) {sqrt(var(x)/length(x)) }


#calc non-parametric power using Monte Carlo methods
#Below based on code from Dr. Loladze http://elifesciences.org/content/3/e02245 
power = function(sample1, sample2, reps=500, size=10) {
    results  <- sapply(1:reps, function(r) {
        resample1 <- sample(sample1, size=size, replace=TRUE) 
        resample2 <- sample(sample2, size=size, replace=TRUE) 
        test <- wilcox.test(resample1, resample2, paired=FALSE, correct=TRUE, exact=FALSE)
        test$p.value
    })
    sum(results<0.05)/reps
}

#for manually setting width of troublesome kable tables
html_table_width <- function(kable_output, width){
  width_html <- paste0(paste0('<col width="', width, '">'), collapse = "\n")
  sub("<table>", paste0("<table>\n", width_html), kable_output) }


#calc Cohen's D
cohen_d <- function(m1,m2,s1,s2){  
  spo <- sqrt((s1**2 + s2**2)/2)
  d <- (m1 - m2)/spo
  effsi <- d / sqrt((d**2)+4)
  ret <- list("d" = d, "effectsi" = effsi)
  return(ret) }
  
#given two list, find members NOT in BOTH  
excluded <- function(l1,l2){
  exc <- c()
  exc <- setdiff(l1,l2)
  exc <- c(exc, setdiff(l2,l1))
  exc <- unique(exc)
  return(exc)
}






summ.lm <- function (x, digits = max(3L, getOption("digits") - 3L), symbolic.cor = x$symbolic.cor, 
    signif.stars = getOption("show.signif.stars"), ...) 
{
    cat("\nCall:\n", paste(deparse(x$call), sep = "\n", collapse = "\n"), 
        "\n\n", sep = "")
    resid <- x$residuals
    df <- x$df
    rdf <- df[2L]
    cat(if (!is.null(x$weights) && diff(range(x$weights))) 
        "Weighted ", "Residuals:\n", sep = "")
    if (rdf > 5L) {
        nam <- c("Min", "1Q", "Median", "3Q", "Max")
        rq <- if (length(dim(resid)) == 2L) 
            structure(apply(t(resid), 1L, quantile), dimnames = list(nam, 
                dimnames(resid)[[2L]]))
        else {
            zz <- zapsmall(quantile(resid), digits + 1L)
            structure(zz, names = nam)
        }
        print(rq, digits = digits, ...)
    }
    else if (rdf > 0L) {
        print(resid, digits = digits, ...)
    }
    else {
        cat("ALL", df[1L], "residuals are 0: no residual degrees of freedom!")
        cat("\n")
    }
    if (length(x$aliased) == 0L) {
        cat("\nNo Coefficients\n")
    }
    else {
        if (nsingular <- df[3L] - df[1L]) 
            cat("\nCoefficients: (", nsingular, " not defined because of singularities)\n", 
                sep = "")
        else cat("\nCoefficients:\n")
        coefs <- x$coefficients
        if (!is.null(aliased <- x$aliased) && any(aliased)) {
            cn <- names(aliased)
            coefs <- matrix(NA, length(aliased), 4, dimnames = list(cn, 
                colnames(coefs)))
            coefs[!aliased, ] <- x$coefficients
        }
        printCoefmat(coefs, digits = digits, signif.stars = signif.stars, 
            na.print = "NA", ...)
    }
    cat("\nResidual standard error:", format(signif(x$sigma, 
        digits)), "on", rdf, "degrees of freedom")
    cat("\n")
    if (nzchar(mess <- naprint(x$na.action))) 
        cat("  (", mess, ")\n", sep = "")
    if (!is.null(x$fstatistic)) {
        cat("Multiple R-squared: ", formatC(x$r.squared, digits = digits))
        cat(",\tAdjusted R-squared: ", formatC(x$adj.r.squared, 
            digits = digits), "\nF-statistic:", formatC(x$fstatistic[1L], 
            digits = digits), "on", x$fstatistic[2L], "and", 
            x$fstatistic[3L], "DF,  p-value:", format.pval(pf(x$fstatistic[1L], 
                x$fstatistic[2L], x$fstatistic[3L], lower.tail = FALSE), 
                digits = digits))
        cat("\n")
    }
    correl <- x$correlation
    if (!is.null(correl)) {
        p <- NCOL(correl)
        if (p > 1L) {
            cat("\nCorrelation of Coefficients:\n")
            if (is.logical(symbolic.cor) && symbolic.cor) {
                print(symnum(correl, abbr.colnames = NULL))
            }
            else {
                correl <- format(round(correl, 2), nsmall = 2, 
                  digits = digits)
                correl[!lower.tri(correl)] <- ""
                print(correl[-1, -p, drop = FALSE], quote = FALSE)
            }
        }
    }
    cat("\n")
    invisible(x)
}

#pretty plots on same page
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}




#### load data
# seasons clients 2017, master version, as of 22 Feb 17
#read in a few lines to get good col names
titles <- read.csv("Season Clients Master_20170222-122327.csv", nrows = 100)
#read in actual data with the faster read_csv 
supdat <- read_csv("Season Clients Master_20170222-122327.csv", trim_ws = TRUE)
colnames(supdat) <- colnames(titles)
storedat <- supdat

##colnames(supdat)
# household count data by village / site, from kenley jones
#pop <- read_excel(paste(dd, "Village Household Survey Results.xlsx", sep = "/"),
#    sheet="Village List")

# site designations (T vs C)
TC <- read_csv("Non-Compulsory Maize Trial - Treatments by Site.csv" , trim_ws = TRUE, skip=3)
##supdat$Site
#begin dropping
supdat <- subset(supdat, DistrictName != "KENYA STAFF")

#start cleaning and prepping data for analysis

supdat$semi.name <- paste(supdat$DistrictName, supdat$SiteName,sep = "-")
TC$semi.name <- paste(TC$District,TC$Site, sep="-")
TC.C <- subset(TC, Treatment == "C" )
TC.T <- subset(TC, Treatment == "T" )

#slim down data to just the sites we care about by T and C
Cdata <- subset(supdat, semi.name %in% TC.C$semi.name)
Tdata <- subset(supdat, semi.name %in% TC.T$semi.name)
#manually inspect colnames to match to features of interest:
#NB for both new and ret clients
#recode all maize choices as 1 and no choice as 0

Cdata$maize.adoption <- Cdata$X2017..Long.Rain_Maize.Maize.Seed.choice
Tdata$maize.adoption <- Tdata$X2017..Long.Rain_Maize.Maize.Seed.choice

Cdata$maize.adoption[is.na(Cdata$maize.adoption)] <- as.numeric(0.0)
Cdata$maize.adoption<- as.numeric(Cdata$maize.adoption )
Cdata$maize.adoption[is.na(Cdata$maize.adoption)] <- as.numeric(1.0)

Tdata$maize.adoption[is.na(Tdata$maize.adoption)] <- as.numeric(0.0)
Tdata$maize.adoption <- as.numeric(Tdata$maize.adoption)
Tdata$maize.adoption[is.na(Tdata$maize.adoption)] <- as.numeric(1.0)

#solar #sunking only??
Cdata$solar <- Cdata$X2017..Long.Rain_Sun.King.Home.qty+ Cdata$X2017..Long.Rain_Sun.King.Mobile.qty+ Cdata$X2017..Long.Rain_Sun.King.Pro.II.qty + Cdata$X2017..Long.Rain_Sun.King.Solo.qty
Tdata$solar <-Tdata$X2017..Long.Rain_Sun.King.Home.qty+ Tdata$X2017..Long.Rain_Sun.King.Mobile.qty+ Tdata$X2017..Long.Rain_Sun.King.Pro.II.qty + Tdata$X2017..Long.Rain_Sun.King.Solo.qty


Cdata$TC <- 0
Tdata$TC <- 1
supdat <- rbind(Cdata,Tdata)

#newmember
supdat$NewMember[supdat$NewMember == "False"] <- 0
supdat$NewMember[supdat$NewMember == "True"] <- 1
supdat$NewMember <- as.numeric(supdat$NewMember)
#adoption = binary
#beans
supdat$beans <- supdat$X2017..Long.Rain_New.Bean.Variety_Credit + supdat$X2017..Long.Rain_Bean.Intercrop_Credit
supdat$beans[supdat$beans >0] <- 1


 #data at individ level
#rejoin dataframe with new flag

indidat <-supdat
indidat$TC[indidat$TC == 0] <- "Control"
indidat$TC[indidat$TC == 1] <- "Treated"
supdat$TC <- as.numeric(supdat$TC)

# #merge groups into unique sites
siteTC <- supdat %>% group_by(semi.name) %>% summarise_each(funs(mean))
supdat <- siteTC # data at site level
#recode treat control into strings
supdat$TC[supdat$TC == 0] <- "Control"
supdat$TC[supdat$TC == 1] <- "Treated"

#add district back in. This is a rather wierd way to do this, admittedly. 
x<-  data.frame(do.call('rbind', strsplit(as.character(supdat$semi.name),'-',fixed=TRUE)))
supdat$District <- x$X1
head(supdat[c("District","semi.name")],n=50)
#final check
table(supdat$TC)


```

# Summary
## Overview

The Non-Compulsory Maize Trial was run during enrollment for the 2017 core program in Kenya. The trial aimed to determine the effects of One Acre Fund’s shift in policy to make maize an optional rather than compulsory product for clients, a change that was first implemented in the 2016 season. We are primarily interested in the effects of this policy change on program enrollment, maize adoption, and overall impact. In this trial, the treatment group had to purchase at least a 1/4 acre of maize in order to enroll in the program. 

You can find the full document set [here]( https://drive.google.com/open?id=0B3_WyX1D8qwMNTk0VFhoRk5SYlE "Link")

In this trial we are using a very generous p-value cut-off at 0.05. However, it should be noted that under proper statistical testing this would need to be adjusted for the number of hypothesis tests between the groups. E.g. if there are 11 hypotheses to be tested then under the Bonferroni correction, our p-value should actually be 0.05/11 = 0.005. Failure to do this will result in an inflated false discovery rate (FDR - essentially how many of our "discoveries" are actually false), however this is not an issue here due to a discovery rate of zero. 


## Main hypotheses
* H1	Making the maize product compulsory will result in higher maize adoption (measured both as average maize acres per farmer and the percent of farmers who chose to purchase any maize)
 
* H2	Making the maize product compulsory will result in lower program enrollment (measured both as average clients per site and the percent of total households in a site that join One Acre Fund)
 
* H3	Making the maize product compulsory will result in higher transaction size (measured as the average client loan size)
 
* H4	Making the maize product compulsory will result in higher program impact (measured as an average of what each client purchased multiplied by the impact of those products)

##Secondary hypotheses
* H5	Average Acres of Maize Purchased (both overall and broken out by new and returning clients)
 
* H6	Average Percent of 1AF Clients Adopting Maize (both overall and broken out by new and returning clients)
 
* H7	Average Clients/Site (both overall and broken out by new and returning clients)
 
* H8	Average 1AF Client Density (% total households joining 1AF) - (this was since removed as a hypothesis)
 
* H9	Average Transaction Size (both overall and broken out by new and returning clients)
 
* H10	Average Program Impact (both overall and broken out by new and returning clients)
 
* H11 Average Adoption Rates of Key Products (solar lights, cookstoves, beans)

## Main takeaways

>*	For all hypotheses tested, no signficant relationships were observed between treatment and response (summary [here](#summaryr) ) 

*	Poor trial design has meant that all hypotheses, except maize adoption, were significantly underpowered. Ideal power is 0.8, most hypotheses here have power 0.05-0.4 under non-normal response distributions. The impact of low-power is three fold:

  + 1) It reduces the liklihood of actually detecting an effect which is real.

  + 2) It increases the risk that any detected effect is false (this is via the positive predictive value (PPV), which is the probability that a detected effect represents a true effect. For example, of our 5 observations (maize adoption etc) let us assume only 1 is actually true (i.e. truly different between treated and control = 1 / (5 − 1) = 0.25), if our studies have 20% power (0.2), then PPV = 0.20 × 0.25 / (0.20 × 0.25 + 0.05) =  0.50. I.e. only one half of our claimed discoveries would actually be true). 

  + 3) the estimate of the effect of treatment is likely to be over exaggerated (this is known as the [Winner's curse](https://blogs.ncl.ac.uk/dennislendrem/2015/12/20/power-sample-size-and-the-winners-curse/ "Link") )

### How to prevent poor statistical power
Statistical power calculations require 3 things, an estimate of the size of the effect (this is often not a raw number, but standardized into standard deviations from the mean), an estimate of error (from pilots or historical data) and a proposed sample size. Several online calculators exist to streamline this process (e.g. [here](http://powerandsamplesize.com/ "Power") ), however these calculations often assume a normal distribution of data (a nice smooth bell curve). It is very common for One Acre Fund data to be non-normal; this is because many of our metrics either push at the 0 or 100% bounds, or reveal some underlying inequality (e.g. maize acreage seems to have 2 peaks, representing two clusters of farmers). In these cases the online calculators are essentially useless, and another approach must be taken to calculate power. As noted above, the impact of low power is fatal to most studies, and often means that resources invested in a trial have no return. 

We can look at how to calculate statistical power for non-normal samples with the following relevant worked example: control group bean adoption was ~ `r round(100*mean(subset(supdat,TC=="Control")$beans),0)` % (this was known pre-trial). As mentioned, percentages pushing at the 0 or 100 bounds are very often non-normal (as is the case here, see histograms below). Hence, statistical tests and power calculation designed for non-normal distributions are required.




```{r, sim power}

#Hence, a non-parametric statistical power calculation would need to be performed (essentially; a Monte Carlo method of calculating power against a simulated data set to find the proportion of observations that result in a p value of <0.05).

t <- "beans"
fakedat <- subset(supdat,TC=="Control")[[t]]
xi<-0.0175
for( i in 1:length(fakedat)){
  if(fakedat[i]<(1-xi)){
    fakedat[i] = fakedat[i] +xi
  } }
tp <- power( subset(supdat,TC=="Control")[[t]] , fakedat , reps=100, size= length(subset(supdat, TC=="Control")[[t]] ))
inp <- cohen_d(mean(subset(supdat,TC=="Control")[[t]]),mean(fakedat),sd(subset(supdat,TC=="Control")[[t]]),sd(fakedat))
pt <- pwr.2p.test(h = inp$effectsi , n = length(subset(supdat,TC=="Control")[[t]]), sig.level = 0.05, power = NULL)

```

If we run both the normal (invalid) power calculation, and the non-normal (valid) power calculation, and simulate that the treatment group would expect to see, at max, a 25% increase in bean adoption, we will yield a power of `r round(pt$power,3)` for normally distributed data and `r round(tp,3)` for non-normal data. These powers are clearly insufficient for a robust analysis that is able to distinguish between there being no actual effect, and just being unable to detect an effect. This tells us we will need a significantly larger sample size to probe the treatment of interest. The simplest way to find the right sample size is to repeat this calculation with increasing sample sizes until our non-normal power is in excess of 0.8.  

Going forward, I have provided a short R script for the easy calculation of statistical powers under normal and non-normal regimes ([here](  https://drive.google.com/open?id=0B3_WyX1D8qwMYVI1MnlBYlNCQ1k  "Link")), this will soon be expanded into an app for easy use. Feel free to [email](mike.barber@oneacrefund.org, "Mike's email") me for more information and guidance on using these scripts. 

# Data analysis
## Data preparation
Adoption data was binarized into 1s and 0s from either product credit or quantity columns in [Season Clients master](https://drive.google.com/open?id=0B3_WyX1D8qwMOXNBclh6TkV1bmc "dataset"). Farmers were then summarised into groups, leading the binarized outcome to become a fraction between 0 and 1. Site names were matched to the treatment/control list provided in [treatment lists](https://drive.google.com/a/oneacrefund.org/file/d/0B3_WyX1D8qwMOEZKbDRhNmt5ck0/view?usp=sharing "Treatment and control lists").



## Testing the main hypotheses
For an RCT trial we can directly compare treatment and control groups to pull out statistical relationships. As a first pass let's look at some of the differences between treatment and control groups as a whole, including statistical tests in our outputs.


```{r, inspect data}
#plot some histos around key observables
# ggplot(NULL, aes(x=X2017..Long.Rain_Maize.acres)) + geom_density(data=Cdata , bw=0.1, colour='blue')  + geom_density(data=Tdata , bw=0.1, colour='red')  + xlab("LR2017 maize acres") + geom_text(data=Tdata,x=0.1, y=1.7, label="Control",colour='blue') + geom_text(data=Tdata,x=0.1, y=1.6, label="Treatment" , colour='red')


h1 <- ggplot(supdat, aes(x=X2017..Long.Rain_Maize.acres,color=TC)) + geom_density(bw=0.05) +ggtitle("acres of maize")
h2 <- ggplot(supdat, aes(x=maize.adoption,color=TC)) + geom_density(bw=0.1) +ggtitle("maize adoption") 
h3 <- ggplot(supdat, aes(x=beans,color=TC)) + geom_density(bw=0.05)  +ggtitle("bean adoption")
h4 <- ggplot(supdat, aes(x=solar,color=TC)) + geom_density(bw=0.05)  +ggtitle("solar adoption")
h5 <- ggplot(supdat, aes(x=X2017..Long.Rain_Envirofit.Cookstove.qty,color=TC)) + geom_density(bw=0.05) +ggtitle("cookstove adoption")
h6 <- ggplot(supdat, aes(x=TotalCredit,color=TC)) + geom_density(bw=300) +ggtitle("Total credit")

#qq=qqnorm(supdat$TotalCredit, plot.it=FALSE)
multiplot(h1,h2,h3,h4,h5,h6,  cols=2)

qq=qqnorm(supdat$TotalCredit, plot.it=FALSE)

  
    
#qqnorm(supdat$X2017..Long.Rain_Maize.acres)
#qqline(supdat$X2017..Long.Rain_Maize.acres)

#ggplot(supdat, aes(x=X2017..Long.Rain_Maize.acres,color=TC)) + geom_histogram(fill="white",alpha=0.2)

```

At a site level, we can see that few of the distributions look normal (perhaps with the exception of total credit, with a QQplot R2 of `r round(cor(qq$x,qq$y),3) `). This lack of normality means we will need to perform non-parametric testing between control and treatment to identify significant differences. The non-parametric test employed here is the Mann-Whitney U test (aka. Wilcoxon ranked sum). Such a test on maize acreage gives an output like the following:

```{r, Wil-maize, echo= FALSE, results='asis' }
#wilcox.test(Cdata$X2017..Long.Rain_Maize.acres, Tdata$X2017..Long.Rain_Maize.acres ,paired=FALSE, correct=TRUE)
#maize site

xa <- wilcox.test(subset(supdat, TC=="Control")$X2017..Long.Rain_Maize.acres, subset(supdat, TC=="Treated")$X2017..Long.Rain_Maize.acres ,paired=FALSE, correct=TRUE)



wiltab <- data.frame("Hypothesis" = "Maize acreage is different for treatment and control groups", "p-value" = round(xa$p.value,2))
kable(wiltab, format='html', booktabs=TRUE) %>%  html_table_width(c(400,300))   
   
```
``
    
      
      
This test gives a p-value of `r round(wilcox.test(subset(supdat, TC=="Control")$X2017..Long.Rain_Maize.acres, subset(supdat, TC=="Treated")$X2017..Long.Rain_Maize.acres ,paired=FALSE, correct=TRUE)$p.value,2)`. P-values above our threshhold of 0.05 are indicative that an observation is likely due to chance, and that there is no significant difference between treatment and control farmers for this hypothesis.

Let's look at some simple hypothesis testing and descriptive statistics for the other key hypothesis before we do a deeper dive:


```{r, Wil-bars, echo=FALSE }
#new stats power

#plot with bootstrapped error bars i.e. nonparametric
#need non normal power calcs too - use a bootstrap method
#note, wilcoxon test AKA Mann-Whitney U test is for non-para testing

t <- "maize.adoption"
tp <- power( subset(supdat,TC=="Control")[[t]] , subset(supdat,TC=="Treated")[[t]] , reps=1000, size= length(subset(supdat, TC=="Control")[[t]] ))

# t.test(subset(supdat,TC=="Control")[[t]],subset(supdat,TC=="Treated")[[t]], paired=FALSE, var.equal=FALSE)
# effsi <- cohen_d(mean(subset(supdat,TC=="Control")[[t]]), mean(subset(supdat,TC=="Treated")[[t]]), sd(subset(supdat,TC=="Control")[[t]]), sd(subset(supdat,TC=="Treated")[[t]]))
# effsi$effectsi
# pwr.2p.test(h=effsi$effectsi, n= length(subset(supdat,TC=="Treated")[[t]]), sig.level = 0.05)


g1 <- ggplot(supdat, aes(TC, maize.adoption)) +  stat_summary(fun.y = mean, geom = "bar") +  stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width=0.25)  + ggtitle("Maize adoption")  
g1 <- add_sub(g1, paste("stat power ~ ",round(tp,3),"\n","Wilcoxon P-value:",round(wilcox.test(subset(supdat, TC=="Control")[[t]], subset(supdat, TC=="Treated")[[t]]  ,paired=FALSE, correct=TRUE)$p.value,3) ))


#same for beans
t <- "beans" 
tp <- power( subset(supdat,TC=="Treated")[[t]] , subset(supdat,TC=="Control")[[t]] , reps=1000, size= length(subset(supdat, TC=="Control")[[t]] ))

g2 <- ggplot(supdat, aes(TC,beans)) + stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width=0.25) + ggtitle("Bean adoption")  
g2 <- add_sub(g2, paste("stat power ~ ",round(tp,3),"\n","Wilcoxon P-value:",round(wilcox.test(subset(supdat, TC=="Control")[[t]], subset(supdat, TC=="Treated")[[t]]  ,paired=FALSE, correct=TRUE)$p.value,3) ))


#same for cookstoves
t <- "X2017..Long.Rain_Envirofit.Cookstove.qty" 
tp <- power( subset(supdat,TC=="Treated")[[t]] , subset(supdat,TC=="Control")[[t]] , reps=1000, size= length(subset(supdat, TC=="Control")[[t]] ))

g3 <- ggplot(supdat, aes(TC, X2017..Long.Rain_Envirofit.Cookstove.qty)) + stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width=0.25) + ggtitle("Cookstove adoption") + ylab("Cookstove") 
g3 <- add_sub(g3, paste("stat power ~ ",round(tp,3),"\n","Wilcoxon P-value:",round(wilcox.test(subset(supdat, TC=="Control")[[t]], subset(supdat, TC=="Treated")[[t]]  ,paired=FALSE, correct=TRUE)$p.value,3) ))

#same for solar
t <- "solar" 
tp <- power( subset(supdat,TC=="Treated")[[t]] , subset(supdat,TC=="Control")[[t]] , reps=1000, size= length(subset(supdat, TC=="Control")[[t]] ))

g4 <- ggplot(supdat, aes(TC, solar)) + stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width=0.25)  + ggtitle("Solar adoption") 

g4 <- add_sub(g4, paste("stat power ~ ",round(tp,3),"\n","Wilcoxon P-value:",round(wilcox.test(subset(supdat, TC=="Control")[[t]], subset(supdat, TC=="Treated")[[t]]  ,paired=FALSE, correct=TRUE)$p.value,3) ))


#loan size
t <- "TotalCredit"
#normal data
inp <- cohen_d(mean(subset(supdat,TC=="Control")[[t]]),mean(subset(supdat,TC=="Treated")[[t]]),sd(subset(supdat,TC=="Control")[[t]]),sd(subset(supdat,TC=="Treated")[[t]]))
tp <- pwr.2p.test(h = inp$effectsi , n = length(subset(supdat,TC=="Control")[[t]]), sig.level = 0.05, power = NULL)$power

g5 <- ggplot(supdat, aes(TC, TotalCredit)) + stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width=0.25) + ggtitle(t) 
#g5 <- g5 + geom_text(aes(x=1,y=1.2*mean(supdat[[t]])) , label=(paste("stat power ~ ",round(tp,3) )))
g5 <- add_sub(g5, paste("stat power ~ ",round(tp,3),"\n","Wilcoxon P-value:",round(wilcox.test(subset(supdat, TC=="Control")[[t]], subset(supdat, TC=="Treated")[[t]]  ,paired=FALSE, correct=TRUE)$p.value,3) ))

#clients per site
#loop over and get numbers per site (important for error calcs)

C.site.num <-c(NULL)
T.site.num <- c(NULL)
Cnom <- c()
Tnom <- c()

for(i in unique(Cdata$semi.name)){
  da <- subset(Cdata, semi.name==i)
  len <- length(unique(da$GlobalClientID))
  C.site.num <- c(C.site.num, len) 
  Cnom <- c(Cnom, i)
    }

for(i in unique(Tdata$semi.name)){
  da <- subset(Tdata, semi.name==i)
  len <- length(unique(da$GlobalClientID))
  T.site.num <- c(T.site.num, len) 
  Tnom <- c(Tnom, i)
}

#convert to df and bind
temp <- data.frame("Nsites" = C.site.num , TC = "Control", "semi.name" = Cnom )
temp2 <- data.frame("Nsites" = T.site.num , TC = "Treated", "semi.name" = Tnom )
temp <- rbind(temp, temp2)
msite <- merge(temp, supdat, by= "semi.name")


t <- "Nsites"
tp <- power( subset(temp,TC=="Treated")[[t]] , subset(temp,TC=="Control")[[t]] , reps=1000, size= length(subset(temp, TC=="Control")[[t]] ))
g6 <- ggplot(temp, aes(TC, Nsites)) + stat_summary(fun.y = mean, geom = "bar") + stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width=0.25) + ggtitle("Average clients per site")
g6 <- add_sub(g6, paste("stat power ~ ",round(tp,3),"\n","Wilcoxon P-value:",round(wilcox.test(subset(temp, TC=="Control")[[t]], subset(temp, TC=="Treated")[[t]]  ,paired=FALSE, correct=TRUE)$p.value,3) ))


plot_grid(g1,g2 , ncol = 2, nrow = 1)
plot_grid(g3,g4 , ncol = 2, nrow = 1)
plot_grid(g5,g6 , ncol = 2, nrow = 1)




```

The above plots show the mean (bar height), confidence intervals (errorbars), Wilcoxon derived p-value and the Monte Carlo derived statistical power. A low P-value represents increased confidence that a finding is 'real' as opposed to just noise. Likewise, a higher statistical power means we can be sure that we are able to detect any difference in the first place. 

Please note that all but one of the main hypotheses are significantly underpowed (ideal power is ~0.8). This means that even if treatment were effecting the above measurables, we would be unable to detect it. The only significant difference observed was in maize adoption, however, regression analysis suggests that this is due to non-identical treatment/control sites rather than a genuine impact (see below).




# Regression analysis
To deconvolute various factors we can run a regression analysis. This will allow us to examine, for example, the effect of treatment when other factors are taken into account (district level effects, average enrolled seasons etc.). Note that we are here using average total enrolled seasons as a more accurate proxy for new client binary responses, this will allow us to calculate the effect of every additional season on responses. 
 
## Simple regression results
Let us first examine the plots from a simple linear regression on maize acreage:

### Maize acreage (H1 H5)
```{r, maize_acre breakdown}

acre.fit <-lm(formula = X2017..Long.Rain_Maize.acres ~ TotalEnrolledSeasons + TC + District, data=supdat)
#summary(acre.fit)$adj.r.squared
par(mfrow = c(2, 2))
plot(acre.fit)


```

Above we see 4 plots:

* The plot in the upper left shows the residual errors plotted versus their fitted values. The residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points. 
* The plot in the upper right shows the distribution of residuals vs. a normal distribution, devaitions from this diagonal line suggest we are violating linear regression assumptions (and therefore cannot trust the outputs), note here that we have several outliers at either extreme of the diagonal. 
* The bottom left plot is the scale-location plot and shows the square root of the standardized residuals (sort of a square root of relative error) as a function of the fitted values. Again, there should be no obvious trend in this plot. 
* Finally the bottom right plot measures the impact of each point on our final output (leverage) vs the error of the point. 


We can see from these plots that a standard linear regression is unlikely to be accurate due to the presence of outliers on the QQplot and the leverage plot. We can therefore run a robust regression to better estimate regression coefficients. A robust regression will give less weight to the outlying points, reducing their impact on our results:



```{r, lms}
acre.rob <-lmrob(formula = X2017..Long.Rain_Maize.acres ~ TC  + TotalEnrolledSeasons + District , data=supdat)

#summary(acre.rob)$adj.r.squared
par(mfrow = c(3, 2))
#plot(acre.rob, which=c(1:4))
plot(acre.rob)


```
 
 
 Re-examining the robust regression plots we see that the trend line for residuals vs fitted values is flat (good). We can now look at the regression coefficients:
```{r, out1, results='asis'}
stargazer(acre.rob, type = "html", omit="District",  covariate.labels = c("Treatment", "OAF Seasons"))
```
Here we have a table of terms and constants with confidence intervals in brackets. A positive term is iterpreted such that an increase in 1 unit of x (eg. 1 more season enrolled) increases the y outcome by the noted amount. Additionally, the presence of 2 or more asterix indicated statistical significance (the coefficient is significantly different from zero). For "Treatment", the coefficient represents the difference between control and treatment group. 

The robust regression for maize acreage fails to find any significant relationship between treatment and acreage. However, we do find a sigificant relationship between new and returning clients (expressed as total enrolled seasons). Every additional season enrolled increases the acreage devoted to maize by 0.06. 

We can likewise run robust regression on the remaining observables (see plots in appendix).

### Bean, cookstove and solar adoption (H11)
```{r, beanreg}

supdat <- Filter(function(x)!all(is.na(x)), supdat)
##

#non normal
acre.bean <-lmrob(formula = beans ~ TC + TotalEnrolledSeasons  + District, data=supdat)
#stargazer(acre.bean, type = "text")
```


```{r, stovereg}
##
#non normal
#sqrt X2017..Long.Rain_Envirofit.Cookstove.qty
supdat$stove.sqrt <- sqrt(supdat$X2017..Long.Rain_Envirofit.Cookstove.qty)
acre.stove <-lmrob(formula = stove.sqrt  ~ TC + TotalEnrolledSeasons + District, data=supdat)
#stargazer(acre.stove, type = "text")
```



```{r, solarreg}
##
acre.solar <-lmrob(formula = solar ~ TC + TotalEnrolledSeasons + District, data=supdat)
#stargazer(acre.solar, type = "text")

```

```{r, starg1, results='asis'}
stargazer(acre.bean, acre.stove, acre.solar, type = "html", omit="District", covariate.labels = c("Treatment", "OAF Seasons"), dep.var.labels = c("Beans", "Stove", "Solar")) 

```

Observing bean adoption, neither number of enrolled seasons or treatment are significant. 

We transform the cookstove data to it's square root to acheive normally distributed residuals. Once again we find that treatment has no detectable effect, however total enrolled seasons is significant at the 0.01 p-level. 

Solar adoption is also not significantly (p<0.05) correlated with treatment or total enrolled seasons. Note that we will not accept a 0.1 p-value as significant due to the [multiple comparison problem]( https://en.wikipedia.org/wiki/Multiple_comparisons_problem "wiki link").



### Maize adoption (H1/H6)
Maize adoption is skewed heavily toward 100% adoption, for this reason we will employ a logit transformation of the response variable to ensure normally distributed residuals (see appendix for untransformed maize adoption regression):


```{r, maizreg}
#trans data to make residuals normalish
supdat$logitmaize.adoption <- logit(supdat$maize.adoption, percents=FALSE, adjust = -0.05)
acre.maizchoice <-lmrob(formula = logitmaize.adoption ~ TC + TotalEnrolledSeasons + District, data=supdat)
acre.maiz <-lmrob(formula = logitmaize.adoption ~ TC, data=supdat)

qp=qqnorm(stdres(acre.maizchoice), plot.it=FALSE)
x <- cor(qp$x,qp$y)
#mean(subset(supdat, TC=="Control")$TotalEnrolledSeasons)
#mean(subset(supdat, TC=="Treated")$TotalEnrolledSeasons)



par(mfrow = c(2, 2))
plot(acre.maizchoice, which=c(1:4))

#paste("R2:",cor(qp$x,qp$y))

# b <- c(-0.2,-0.1,-0.05,-0.01,0.01,0.05,0.1,0.2)
# 
# for(i in b){
#   supdat$testy <- logit(supdat$maize.adoption, percents=FALSE, adjust = i)
# 
#   xi <- c()
#   acre.maizchoice <-lmrob(formula = testy ~ TC + TotalEnrolledSeasons + District, data=supdat)
#   qp=qqnorm(stdres(acre.maizchoice), plot.it=FALSE)
#   print(paste(i,"R2:",cor(qp$x,qp$y)))
# 
# }

  
```

This transformation yields normal looking residuals with an R2 against a normal distribution of `r round(x,3)`.

We can now examine the regression output:

```{r, maizout, results='asis'}
stargazer(acre.maizchoice, type = "html", covariate.labels = c("Treatment", "OAF Seasons"))
x<- round(wilcox.test(subset(supdat, TC=="Control")$TotalEnrolledSeasons, subset(supdat, TC=="Treated")$TotalEnrolledSeasons ,paired=FALSE, correct=TRUE)$p.value,3)

```
    
    
Whilst our simple analysis identified treatment as significantly correlated with, when we correct for other factors this significance disappears (see appendix for a robust regression of maize adoption against only Treatment). However total enrolled seasons is clearly significant. This suggests that the difference in maize adoption between treatment and control sites was not due to treatment, but other factors. This nullifies our only significant finding from this analysis. 

### Total loan size (H3/H9)
```{r, creditreg, results='asis'}
##
acre.cred <-lmrob(formula = TotalCredit ~ TC + TotalEnrolledSeasons + District, data=supdat)
stargazer(acre.cred, type = "html", omit="District", covariate.labels = c("Treatment", "OAF Seasons"))


#We can further break this down by new or returning member; `r round(mean(subset(Cdata, NewMember=="True")$X2017..Long.Rain_Maize.acres),2)` and  `r round(mean(subset(Tdata, NewMember=="True")$X2017..Long.Rain_Maize.acres),2)` acres for new farmers from control and treatment group respectively, and `r round(mean(subset(Cdata, NewMember=="False")$X2017..Long.Rain_Maize.acres),2)` and  `r round(mean(subset(Tdata, NewMember=="False")$X2017..Long.Rain_Maize.acres),2)` acres for returning farmers from control and treatment group respectively.
```
For the total credit observable, we again find that treatment has no significant effect on outcomes, however total enrolled seasons correlates well with total credit. For every additional season enrolled, average site level credit increases by 617 KES. 


### Clients per site (H2/H7)
```{r, sitesize, results='asis'}
msite$TC <- supdat$TC
msite$transNclients <- sqrt(msite$Nsites)
nclients <-lmrob(formula = transNclients ~ TC + TotalEnrolledSeasons  + District, data=msite)
stargazer(nclients, type = "html", omit="District", covariate.labels = c("Treatment", "OAF Seasons"))
qn=qqnorm(stdres(nclients), plot.it=FALSE)
x1 <- cor(qn$x,qn$y)
x1 <- round(x1,3)


```

Once again, non-normal residuals means we need to transform our Y-variable. In this case a square root transform yields an R-squared of `r x1` against a normal distribution. Examination of the coefficients reveals no significant relationships were detected. 



# Summary of results {#summaryr}
Overall, this study suffered significantly from a real lack of power, we therefore do not know whether the lack of associations between outcomes and treatment are due to there being no relationship, or just to our inability to detect a relationship. In the "Main Takeaways" section I have outlined in more depth the issues of low power as well as strategies to calculate this a priori. 

```{r, table, echo=FALSE}

Hypo <- c("Treatment affects total maize adoption (H1, H6)","Treatment affects maize acreage (H1, H5)","Treatment will affect total clients per site (H2, H7)","Treatment affects Treatment will affect loan size (H3, H9)","Treatment will affect","Treatment will affect program impact (H4, H10)","Treatment will affect solar, beans, or stove adoption (H11)" )

tab <- data.frame("Hypothesis" = Hypo,"Result" = "No result")

kable(tab)

```


# Appendix

##Program impact hypothesis (H4/H9)
Note that if there are no significant differences between treatment and control adoption of goods, then likewise there will be no significant difference in program impact between treatment and control (a linear transformation of populations cannot yield new populations). 

##Regression terminology:
 _______.

* **95% confidence interval**: The coefficient is just a point estimate of how much change in outcome variable is caused by being in the treatment group. An "X-Y"" 95% confidence error gives us a sense of how certain we are. It basically says that 95% of the time, we would expect that a treatment site performs better than a control site by this amount. The bigger the confidence interval, the more uncertainty we have.

* **P-Value**: This is a measure of statistical significance. It tells you the probability that your null hypothesis (in this case, that the outcome variables in the regression *do not* correlate significantly to treatment) is true. Thus, for our purposes, we want to see very low p-values to believe that there is a strong causal relationship between treatment and outcome variable. The P-value and confidence interval are best interpreted together: if the p-value is pretty low (a general rule of thumb is <0.05), then you should look at the confidence interval and see whether the magnitude of change is meaningful or not.

* **Statistical test**: This is the test used in P-value calculation. Each test comes with certain assumptions which must be met to ensure the P-value and confidence interval output are reliable. The quickest way to see if the test matches the data is by examining the histograms above and reading up on test assumptions on wikipedia. 

 _______.


##Non-robust regression results


```{r, residuals plus}


#residual plots first
#tes <- c("X2017..Long.Rain_Maize.acres", "maize.adoption","X2017..Long.Rain_Bean.Intercrop_Credit", "X2017..Long.Rain_Envirofit.Cookstove.qty","solar", "TotalCredit") 
supdat$stove.sqrt <- sqrt(supdat$X2017..Long.Rain_Envirofit.Cookstove.qty)
#supdat$maize.c <- (supdat$maize.adoption)/mean(supdat$maize.adoption)
#summary(supdat$maize.c)
#summary(supdat$X2017..Long.Rain_Envirofit.Cookstove.qty)
#sqrt the stove to normal resid
#summary(supdat$stove.sqrt)
tes <- c("X2017..Long.Rain_Maize.acres", "maize.adoption","beans", "stove.sqrt","solar", "TotalCredit") 
for(ix in tes){
  tempreg <- NULL
  supdat$newy <- supdat[[ix]]
  tempreg <- lm(formula = newy ~ TC + District + TotalEnrolledSeasons, data=supdat)
  par(mfrow = c(2, 2))
  plot(tempreg,which = c(1:4), caption=ix, main=ix )  
  qqp=qqnorm(stdres(tempreg), plot.it=FALSE)
  #print(ix)
  #print(paste("R2:",cor(qqp$x,qqp$y)))
    
  }  


```


##robust regression with only treatment variable
```{r, residuals}


#residual plots first
tes <- c("X2017..Long.Rain_Maize.acres", "maize.adoption","X2017..Long.Rain_Bean.Intercrop_Credit", "X2017..Long.Rain_Envirofit.Cookstove.qty","solar", "TotalCredit") 
for(ix in tes){
  tempreg <- NULL
  supdat$newy <- supdat[[ix]]
  tempreg <- lmrob(formula = newy ~ TC , data=supdat)
  par(mfrow = c(2, 2))
  plot(tempreg,which = c(1:4), caption=ix, main=ix )    }  
```
##Robust regression coefficients for transformed maize adoption when only treatment is considered as a factor
```{r, robTC, results='asis'}
stargazer(acre.maiz, type="html")
```

##robust regression diagnostic plots for coefficient tables in main body

```{r robust plots}
par(mfrow = c(2, 2))
plot(acre.bean)
par(mfrow = c(2, 2))
plot(acre.stove)
par(mfrow = c(2, 2))
plot(acre.cred) 
par(mfrow = c(2, 2))
plot(acre.maizchoice)
par(mfrow = c(2, 2))
plot(acre.solar)
par(mfrow = c(2, 2))
plot(nclients)

```